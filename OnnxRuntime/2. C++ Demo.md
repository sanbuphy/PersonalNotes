[ONNX runtime demo 官方仓库](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/c_cxx)

# C++ demo（CPU）
该 demo 假设模型单输入单输出， 静态 shape 输入；

[显示batch demo](https://github.com/microsoft/onnxruntime-inference-examples/blob/main/c_cxx/model-explorer/model-explorer.cpp)

参考 [完整代码](./code_1)

[隐式batch demo](https://github.com/microsoft/onnxruntime-inference-examples/blob/main/c_cxx/model-explorer/batch-model-explorer.cpp)

这两个 demo 将模型运行在 CPU上。

# C++ demo（GPU CUDA）
该 demo 假设模型单输入单输出， 静态 shape 输入；
参考 [完整代码](./code_2)

## 多输入多输出 demo （动态shape）
参考 [完整代码](./code_3)
