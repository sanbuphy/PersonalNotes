{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 快速开始-数据并行\n",
    "\n",
    "## 1.1 版本要求\n",
    "\n",
    "## 1.2 具体步骤\n",
    "'''\n",
    "数据并行非常适合单卡已经能够放得下完整的模型和参数，但希望通过并行来增大 全局数据(global batch)大小来提升训练的吞吐量。\n",
    "\n",
    "与单机单卡的普通模型训练相比，数据并行训练只需要按照如下 5 个步骤对代码进行简单调整即可：\n",
    "\n",
    "导入分布式训练依赖包\n",
    "\n",
    "初始化 Fleet 环境\n",
    "\n",
    "构建分布式训练使用的网络模型\n",
    "\n",
    "构建分布式训练使用的优化器\n",
    "\n",
    "构建分布式训练使用的数据加载器\n",
    "'''\n",
    "# 1.3 完整示例代码\n",
    "# train_with_fleet.py\n",
    "# -*- coding: UTF-8 -*-\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddle.vision.transforms import ToTensor\n",
    "# 一、导入分布式专用 Fleet API\n",
    "from paddle.distributed import fleet\n",
    "# 构建分布式数据加载器所需 API\n",
    "from paddle.io import DataLoader, DistributedBatchSampler\n",
    "# 设置 GPU 环境\n",
    "paddle.set_device('gpu')\n",
    "\n",
    "class MyNet(paddle.nn.Layer):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(MyNet, self).__init__()\n",
    "\n",
    "        self.conv1 = paddle.nn.Conv2D(in_channels=3, out_channels=32, kernel_size=(3, 3))\n",
    "        self.pool1 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = paddle.nn.Conv2D(in_channels=32, out_channels=64, kernel_size=(3,3))\n",
    "        self.pool2 = paddle.nn.MaxPool2D(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = paddle.nn.Conv2D(in_channels=64, out_channels=64, kernel_size=(3,3))\n",
    "\n",
    "        self.flatten = paddle.nn.Flatten()\n",
    "\n",
    "        self.linear1 = paddle.nn.Linear(in_features=1024, out_features=64)\n",
    "        self.linear2 = paddle.nn.Linear(in_features=64, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "epoch_num = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "val_acc_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "def train():\n",
    "    # 二、初始化 Fleet 环境\n",
    "    fleet.init(is_collective=True)\n",
    "\n",
    "    model = MyNet(num_classes=10)\n",
    "    # 三、构建分布式训练使用的网络模型\n",
    "    model = fleet.distributed_model(model)\n",
    "\n",
    "    opt = paddle.optimizer.Adam(learning_rate=learning_rate,parameters=model.parameters())\n",
    "    # 四、构建分布式训练使用的优化器\n",
    "    opt = fleet.distributed_optimizer(opt)\n",
    "\n",
    "    transform = ToTensor()\n",
    "    cifar10_train = paddle.vision.datasets.Cifar10(mode='train',\n",
    "                                           transform=transform)\n",
    "    cifar10_test = paddle.vision.datasets.Cifar10(mode='test',\n",
    "                                          transform=transform)\n",
    "\n",
    "    # 五、构建分布式训练使用的数据集\n",
    "    train_sampler = DistributedBatchSampler(cifar10_train, batch_size, shuffle=True, drop_last=True)\n",
    "    train_loader = DataLoader(cifar10_train, batch_sampler=train_sampler, num_workers=2)\n",
    "\n",
    "    valid_sampler = DistributedBatchSampler(cifar10_test, batch_size, drop_last=True)\n",
    "    valid_loader = DataLoader(cifar10_test, batch_sampler=valid_sampler, num_workers=2)\n",
    "\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "        model.train()\n",
    "        for batch_id, data in enumerate(train_loader()):\n",
    "            x_data = data[0]\n",
    "            y_data = paddle.to_tensor(data[1])\n",
    "            y_data = paddle.unsqueeze(y_data, 1)\n",
    "\n",
    "            logits = model(x_data)\n",
    "            loss = F.cross_entropy(logits, y_data)\n",
    "\n",
    "            if batch_id % 1000 == 0:\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for batch_id, data in enumerate(valid_loader()):\n",
    "            x_data = data[0]\n",
    "            y_data = paddle.to_tensor(data[1])\n",
    "            y_data = paddle.unsqueeze(y_data, 1)\n",
    "\n",
    "            logits = model(x_data)\n",
    "            loss = F.cross_entropy(logits, y_data)\n",
    "            acc = paddle.metric.accuracy(logits, y_data)\n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "\n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "        print(\"[validation] accuracy/loss: {}/{}\".format(avg_acc, avg_loss))\n",
    "        val_acc_history.append(avg_acc)\n",
    "        val_loss_history.append(avg_loss)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grep: warning: GREP_OPTIONS is deprecated; please use an alias or script\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 -----------  Configuration  ----------------------\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 devices: 0,1\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 elastic_level: -1\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 elastic_timeout: 30\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 gloo_port: 6767\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 host: None\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 ips: None\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 job_id: default\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 legacy: False\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 log_dir: ./log.txt\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 log_level: INFO\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 master: None\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 max_restart: 3\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 nnodes: 1\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 nproc_per_node: None\n",
      "LAUNCH INFO 2023-01-05 08:47:29,802 rank: -1\n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 run_mode: collective\n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 server_num: None\n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 servers: \n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 start_port: 6070\n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 trainer_num: None\n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 trainers: \n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 training_script: train_with_fleet.py\n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 training_script_args: []\n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 with_gloo: 1\n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 --------------------------------------------------\n",
      "LAUNCH INFO 2023-01-05 08:47:29,803 Job: default, mode collective, replicas 1[1:1], elastic False\n",
      "LAUNCH INFO 2023-01-05 08:47:29,808 Run Pod: czpnuj, replicas 2, status ready\n",
      "LAUNCH INFO 2023-01-05 08:47:29,820 Watching Pod: czpnuj, replicas 2, status running\n",
      "grep: warning: GREP_OPTIONS is deprecated; please use an alias or script\n",
      "[2023-01-05 08:47:33,600] [    INFO] distributed_strategy.py:147 - distributed strategy initialized\n",
      "I0105 08:47:33.601104 67527 tcp_utils.cc:181] The server starts to listen on IP_ANY:47228\n",
      "I0105 08:47:33.601258 67527 tcp_utils.cc:130] Successfully connected to 10.75.148.145:47228\n",
      "W0105 08:47:40.265255 67527 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 7.5, Driver API Version: 11.7, Runtime API Version: 11.6\n",
      "W0105 08:47:40.271625 67527 gpu_resources.cc:91] device: 0, cuDNN Version: 8.4.\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "C++ Traceback (most recent call last):\n",
      "--------------------------------------\n",
      "0   paddle::distributed::ProcessGroupNCCL::Barrier(paddle::distributed::BarrierOptions const&)\n",
      "1   paddle::distributed::ProcessGroupNCCL::AllReduce(phi::DenseTensor*, phi::DenseTensor const&, paddle::distributed::AllreduceOptions const&, bool, bool)\n",
      "2   paddle::distributed::ProcessGroupNCCL::RunFnInNCCLEnv(std::function<void (ncclComm*, CUstream_st*)>, phi::DenseTensor const&, paddle::distributed::CommType, bool, bool)\n",
      "3   paddle::distributed::ProcessGroupNCCL::CreateNCCLEnvCache(phi::Place const&, std::string const&)\n",
      "4   ncclCommInitRank\n",
      "\n",
      "----------------------\n",
      "Error Message Summary:\n",
      "----------------------\n",
      "FatalError: `Access to an undefined portion of a memory object` is detected by the operating system.\n",
      "  [TimeInfo: *** Aborted at 1672908462 (unix time) try \"date -d @1672908462\" if you are using GNU date ***]\n",
      "  [SignalInfo: *** SIGBUS (@0x7f43b58b6000) received by PID 67527 (TID 0x7f4453aa3740) from PID 18446744072460394496 ***]\n",
      "\n",
      "LAUNCH INFO 2023-01-05 08:47:44,835 Pod failed\n",
      "LAUNCH ERROR 2023-01-05 08:47:44,836 Container failed !!!\n",
      "Container rank 1 status failed cmd ['/usr/bin/python', '-u', 'train_with_fleet.py'] code -7 log ./log.txt/workerlog.1 \n",
      "env {'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'VSCODE_CWD': '/root/.vscode-server/bin/1.8.401.70.0.1', 'NV_CUDA_COMPAT_PACKAGE': 'cuda-compat-11-6', 'NV_LIBCUBLAS_VERSION': '11.9.2.110-1', 'ICODING_CTOKEN': '50c78b3e51736d9ec973048e3d64da4f', 'PYTHONIOENCODING': 'utf-8', 'LANGUAGE': 'en_US.UTF-8', 'VSCODE_NLS_CONFIG': '{\"locale\":\"en\",\"availableLanguages\":{}}', 'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'HOSTNAME': 'yq01-sys-rpm0420ea02d.yq01.baidu.com', 'GREP_COLOR': '1;31', 'LD_LIBRARY_PATH': '/usr/local/lib/python3.7/dist-packages/cv2/../../lib64:/usr/local/lib/python3.7/dist-packages/cv2/../../lib64:', 'NV_LIBNCCL_PACKAGE_VERSION': '2.12.10-1', 'SHLVL': '4', 'BROWSER': '/root/.vscode-server/bin/1.8.401.70.0.1/bin/helpers/browser.sh', 'OLDPWD': '/root/.iCoding-agent', 'HOME': '/root', 'NV_LIBCUBLAS_DEV_VERSION': '11.9.2.110-1', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn8', 'VSCODE_IPC_HOOK_CLI': '/tmp/vscode-ipc-40d5dde5-9f50-453d-8aca-7352871153aa.sock', 'PYTHONUNBUFFERED': '1', 'APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL': 'true', 'PAGER': 'cat', 'PS1': '\\\\[\\\\033[1;33m\\\\]λ \\\\[\\\\033[1;37m\\\\]\\\\h \\\\[\\\\033[1;32m\\\\]\\\\w \\\\[\\\\033[0m\\\\]', 'ICODING_REMOTE_USERNAME': 'weishengying', 'WITH_GPU': 'ON', 'GOROOT': '/usr/local/go', 'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.12.10-1', 'NV_LIBNPP_PACKAGE': 'libnpp-11-6=11.6.3.124-1', 'QT_QPA_PLATFORM_PLUGIN_PATH': '/usr/local/lib/python3.7/dist-packages/cv2/qt/plugins', 'CUDA_VERSION': '11.6.2', 'NV_CUDNN_PACKAGE': 'libcudnn8=8.4.0.27-1+cuda11.6', 'WITH_AVX': 'ON', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-11-6', 'NVIDIA_REQUIRE_CUDA': 'cuda>=11.6 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 brand=tesla,driver>=450,driver<451 brand=tesla,driver>=460,driver<461 brand=tesla,driver>=470,driver<471', '_': '/root/.vscode-server/bin/1.8.401.70.0.1/node', 'VSCODE_HANDLES_SIGPIPE': 'true', 'NV_LIBCUSPARSE_VERSION': '11.7.2.124-1', 'NV_CUDA_LIB_VERSION': '11.6.2-1', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-11-6=11.6.3.124-1', 'TERM': 'xterm-color', 'NV_NVML_DEV_VERSION': '11.6.55-1', 'ICODING_AGENTID': '301795', 'ICODING_LOCAL_ADDRESS': '10.75.148.145:10001', 'NV_CUDNN_PACKAGE_DEV': 'libcudnn8-dev=8.4.0.27-1+cuda11.6', 'NV_CUDA_CUDART_VERSION': '11.6.55-1', 'ICODING_UTOKEN': 'eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJzVG9rZW4iOiJTVC04MTE1NDMwNTQ5ODIxODQ5NjItam5hR2wtc2Vzc2lvbiIsImZyb21BcHBLZXkiOiJ1dWFwY2xpZW50LTE3LVEyVFcwUHN1WUVsUEFoU0RHd0lUIiwiaXNzIjoidXVhcCIsInBUb2tlbiI6IlBULTgxMTU0MzA1NDk4MjE4NDk2MS0zdTN6NEtFaEdHLXNlc3Npb24iLCJleHAiOjE2NzM2NzY1MDgsImlhdCI6MTY3MTQxNjQ1MiwidHRsIjoiMjI1OTk2NiJ9.HEO3qfz0Rb3jIv4jdDYSSQLbcmCtLucZICnjJcCZplI', 'PATH': '/usr/bin:/root/.vscode-server/bin/1.8.401.70.0.1/bin/remote-cli:/root/.vscode-server/bin/1.8.401.70.0.1/bin:/root/.vscode-server/bin:/home/cmake-3.16.0-Linux-x86_64/bin:/usr/local/gcc-8.2/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/root/gopath/bin', 'NVARCH': 'x86_64', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-11-6=11.9.2.110-1', 'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-11-6', 'VSCODE_AGENT_FOLDER': '/root/.vscode-server', 'ICODING_VERSION': '1.8.401.70.0.1', 'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess', 'GREP_OPTIONS': '--color=auto', 'NV_LIBCUSPARSE_DEV_VERSION': '11.7.2.124-1', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.12.10-1+cuda11.6', 'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev', 'LANG': 'en_US.UTF-8', 'NV_CUDA_CUDART_DEV_VERSION': '11.6.55-1', 'DEBIAN_FRONTEND': 'noninteractive', 'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-11-6=11.9.2.110-1', 'GOPATH': '/root/gopath', 'ELECTRON_RUN_AS_NODE': '1', 'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.12.10-1+cuda11.6', 'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1', 'NV_LIBNPP_VERSION': '11.6.3.124-1', 'NV_NVTX_VERSION': '11.6.124-1', 'NV_CUDNN_VERSION': '8.4.0.27', 'GIT_PAGER': 'cat', 'QT_QPA_FONTDIR': '/usr/local/lib/python3.7/dist-packages/cv2/qt/fonts', 'LC_ALL': 'en_US.UTF-8', 'PWD': '/weishengying/PersonalNotes/PaddlePaddle/Paddle/文档/使用指南/5. 分布式训练', 'CLICOLOR': '1', 'OMP_NUM_THREADS': '1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NCCL_VERSION': '2.12.10-1', 'CUSTOM_DEVICE_ROOT': '', 'NV_LIBNPP_DEV_VERSION': '11.6.3.124-1', 'ICODING_REMOTE_ADDRESS': '10.220.14.197:43126', 'POD_NAME': 'czpnuj', 'PADDLE_MASTER': '10.75.148.145:47228', 'PADDLE_GLOBAL_SIZE': '2', 'PADDLE_LOCAL_SIZE': '2', 'PADDLE_GLOBAL_RANK': '1', 'PADDLE_LOCAL_RANK': '1', 'PADDLE_NNODES': '1', 'PADDLE_TRAINER_ENDPOINTS': '10.75.148.145:47229,10.75.148.145:47230', 'PADDLE_CURRENT_ENDPOINT': '10.75.148.145:47230', 'PADDLE_TRAINER_ID': '1', 'PADDLE_TRAINERS_NUM': '2', 'PADDLE_RANK_IN_NODE': '1', 'FLAGS_selected_gpus': '1'}\n",
      "LAUNCH INFO 2023-01-05 08:47:44,836 ------------------------- ERROR LOG DETAIL -------------------------\n",
      "grep: warning: GREP_OPTIONS is deprecated; please use an alias or script\n",
      "[2023-01-05 08:47:33,653] [    INFO] distributed_strategy.py:147 - distributed strategy initialized\n",
      "I0105 08:47:33.654413 67528 tcp_utils.cc:130] Successfully connected to 10.75.148.145:47228\n",
      "W0105 08:47:40.578568 67528 gpu_resources.cc:61] Please NOTE: device: 1, GPU Compute Capability: 7.5, Driver API Version: 11.7, Runtime API Version: 11.6\n",
      "W0105 08:47:40.584761 67528 gpu_resources.cc:91] device: 1, cuDNN Version: 8.4.\n",
      "\n",
      "\n",
      "--------------------------------------\n",
      "C++ Traceback (most recent call last):\n",
      "--------------------------------------\n",
      "0   paddle::distributed::ProcessGroupNCCL::Barrier(paddle::distributed::BarrierOptions const&)\n",
      "1   paddle::distributed::ProcessGroupNCCL::AllReduce(phi::DenseTensor*, phi::DenseTensor const&, paddle::distributed::AllreduceOptions const&, bool, bool)\n",
      "2   paddle::distributed::ProcessGroupNCCL::RunFnInNCCLEnv(std::function<void (ncclComm*, CUstream_st*)>, phi::DenseTensor const&, paddle::distributed::CommType, bool, bool)\n",
      "3   paddle::distributed::ProcessGroupNCCL::CreateNCCLEnvCache(phi::Place const&, std::string const&)\n",
      "4   ncclCommInitRank\n",
      "\n",
      "----------------------\n",
      "Error Message Summary:\n",
      "----------------------\n",
      "FatalError: `Access to an undefined portion of a memory object` is detected by the operating system.\n",
      "  [TimeInfo: *** Aborted at 1672908462 (unix time) try \"date -d @1672908462\" if you are using GNU date ***]\n",
      "  [SignalInfo: *** SIGBUS (@0x7f171a543000) received by PID 67528 (TID 0x7f17d95a2740) from PID 441724928 ***]\n",
      "\n",
      "LAUNCH INFO 2023-01-05 08:47:46,352 Exit code -7\n"
     ]
    }
   ],
   "source": [
    "# 1.4 分布式启动\n",
    "!export CUDA_VISIBLE_DEVICES=0,1\n",
    "!python -m paddle.distributed.launch --gpus=0,1  --log_dir=./log --run_mode=collective train_with_fleet.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
