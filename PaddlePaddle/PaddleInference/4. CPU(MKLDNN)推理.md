# 1 AnalysisConfig 中与 cpu 推理有关的设置
（ paddle_analysis_config.cc）
1. EnableMKLDNN / enable_mkldnn
2. SetMKLDNNOp / set_mkldnn_op
3. EnableMkldnnQuantizer / enable_quantizer
4. EnableMkldnnInt8 / enable_mkldnn_int8
5. EnableMkldnnBfloat16 / enable_mkldnn_bfloat16
6. DisableMkldnnFcPasses / disable_mkldnn_fc_passes

这六个设置对应 `CpuPassStrategy` 中的设置，EnableMKLDNN，EnableMkldnnQuantizer， EnableMkldnnInt8， EnableMkldnnBfloat16，DisableMkldnnFcPasses

`SetMKLDNNOp`好像没有作用了。（待确认）

Enable 不同的开关，使得在图形优化时会跑不同的pass，具体查看 `CpuPassStrategy`函数中的相关 API 对 `passes_` 的修改。

# 2 mkldnn 量化推理
paddle inference 目前只支持 conv2d，depthwise_conv2d，fused_conv2d， 以及 mul， matmul， matmul_v2 这些带权重的 op 的量化推理， 其余 op 正常走 fp32 推理

当开启 mkldnn 量化推理开关时：`cobfig.enable_mkldnn_int8({"conv2d", ...})`, paddle inference 会跑以下 pass 进行图层面的优化：

```cpp
// paddle_pass_builder.cc 中
void CpuPassStrategy::EnableMkldnnInt8() {
#ifdef PADDLE_WITH_MKLDNN
  if (!use_mkldnn_int8_) {
    passes_.clear();
    passes_.push_back("simplify_with_basic_ops_pass");
    passes_.push_back("quant_dequant_mkldnn_pass");
    passes_.push_back("mkldnn_placement_pass");
    passes_.push_back("constant_folding_pass");
    passes_.push_back("squeeze2_transpose2_onednn_fuse_pass");
    passes_.push_back("layer_norm_fuse_pass");
    passes_.push_back("attention_lstm_fuse_pass");
    passes_.push_back("seqconv_eltadd_relu_fuse_pass");
    passes_.push_back("fc_lstm_fuse_pass");
    passes_.push_back("mul_lstm_fuse_pass");
    passes_.push_back("fc_gru_fuse_pass");
    passes_.push_back("mul_gru_fuse_pass");
    passes_.push_back("multi_gru_fuse_pass");
    passes_.push_back("multi_gru_seq_fuse_pass");
    passes_.push_back("seq_concat_fc_fuse_pass");
    passes_.push_back("gpu_cpu_squeeze2_matmul_fuse_pass");
    passes_.push_back("gpu_cpu_reshape2_matmul_fuse_pass");
    passes_.push_back("gpu_cpu_flatten2_matmul_fuse_pass");
    passes_.push_back("matmul_v2_scale_fuse_pass");
    passes_.push_back("squared_mat_sub_fuse_pass");
    passes_.push_back("is_test_pass");
    passes_.push_back("gpu_cpu_map_matmul_v2_to_mul_pass");
    passes_.push_back("gpu_cpu_map_matmul_v2_to_matmul_pass");
    passes_.push_back("matmul_scale_fuse_pass");
    passes_.push_back("gpu_cpu_map_matmul_to_mul_pass");
    passes_.push_back("repeated_fc_relu_fuse_pass");
    passes_.push_back("depthwise_conv_mkldnn_pass");
    passes_.push_back("conv_bn_fuse_pass");
    passes_.push_back("conv_eltwiseadd_bn_fuse_pass");
    passes_.push_back("conv_affine_channel_mkldnn_fuse_pass");
    passes_.push_back("conv_transpose_bn_fuse_pass");
    passes_.push_back("conv_transpose_eltwiseadd_bn_fuse_pass");
    passes_.push_back("conv_bias_mkldnn_fuse_pass");
    passes_.push_back("conv_transpose_bias_mkldnn_fuse_pass");
    passes_.push_back("conv_elementwise_add_mkldnn_fuse_pass");
    passes_.push_back("conv_activation_mkldnn_fuse_pass");
    passes_.push_back("fc_fuse_pass");
    passes_.push_back("repeated_fc_relu_fuse_pass");
    passes_.push_back("fc_mkldnn_pass");
    passes_.push_back("fc_act_mkldnn_fuse_pass");
    passes_.push_back("fc_elementwise_add_mkldnn_fuse_pass");
    passes_.push_back("matmul_transpose_reshape_mkldnn_fuse_pass");
    passes_.push_back("batch_norm_act_fuse_pass");
    passes_.push_back("softplus_activation_mkldnn_fuse_pass");
    passes_.push_back("compute_propagate_scales_mkldnn_pass");
    passes_.push_back("scale_matmul_fuse_pass");
    passes_.push_back("reshape_transpose_matmul_mkldnn_fuse_pass");
    passes_.push_back("matmul_elementwise_add_mkldnn_fuse_pass");
    passes_.push_back("layer_norm_onednn_optimization_pass");
    passes_.push_back("operator_scale_onednn_fuse_pass");
    passes_.push_back("operator_unsqueeze2_onednn_fuse_pass");
    passes_.push_back("operator_reshape2_onednn_fuse_pass");
    passes_.push_back("cpu_quantize_placement_pass");
    passes_.push_back("cpu_quantize_pass");
    passes_.push_back("cpu_quantize_squash_pass");
    passes_.push_back("int8_scale_calculation_mkldnn_pass");
    passes_.push_back("params_quantization_mkldnn_pass");
  }
  use_mkldnn_int8_ = true;
#else
  use_mkldnn_int8_ = false;
#endif
}
```
这些 pass 主要为:

1. 图层面优化的pass，这些 pass 与硬件无关。
  1.1 如 `simplify_with_basic_ops_passs`是为了去掉 dropout 算子(推理时不需要 dropout);
  
  1.2 以及大量名称中含有`fuse`字段的pass，主要做算子融合操作;

  1.3 `constant_folding_pass` 常量折叠 pass

  1.4 `gpu_cpu_map_matmul_v2_to_mul_pass`, `gpu_cpu_map_matmul_v2_to_matmul_pass`, `gpu_cpu_map_matmul_to_mul_pass`
  做算子替换，也是为了方便替换后做一些算子融合操作。

去除掉上述图层面优化的 pass 之后（保留常量折叠 pass 和 去掉 dropout 算子两个基础优化 pass），剩余的pass为：
```cpp
  if (!use_mkldnn_int8_) {
    passes_.clear();
    passes_.push_back("simplify_with_basic_ops_pass"); // 去掉 dropout 算子
    passes_.push_back("quant_dequant_mkldnn_pass"); // 处理量化模型， 去掉量化算子
    passes_.push_back("mkldnn_placement_pass");
    passes_.push_back("constant_folding_pass"); // 常量折叠
    passes_.push_back("depthwise_conv_mkldnn_pass");
    passes_.push_back("fc_mkldnn_pass");
    passes_.push_back("compute_propagate_scales_mkldnn_pass");
    passes_.push_back("layer_norm_onednn_optimization_pass");
    passes_.push_back("cpu_quantize_placement_pass");
    passes_.push_back("cpu_quantize_pass");
    passes_.push_back("cpu_quantize_squash_pass");
    passes_.push_back("int8_scale_calculation_mkldnn_pass");
    passes_.push_back("params_quantization_mkldnn_pass");
  }
```

在开启 `config.switch_ir_debug(1)`后，每经过一个 pass， 便会将图层面上的改动保存为 model，方便查看该 pass 对模型的修改是否符合预期。
但实际操作过程中，有时发现该功能存在bug（如实际并未修改，但保存的模型显示已经修改了）。故可以在 pass 的 `ApplyImpl`开始和结尾，手动将 graph 保存为 program。如在 `quant_dequant_mkldnn_pass`的执行函数的前后增加如下代码。

```cpp
void QuantDequantMkldnnPass::ApplyImpl(ir::Graph* graph) const {
  {
    ProgramDesc program_desc;
    GraphToProgram(*graph, &program_desc);
    std::string program_bytes = program_desc.Proto()->SerializeAsString();
    std::string program_path = "_before_QuantDequantMkldnnPass.pdmodel";
    std::ofstream file(program_path.c_str(), std::ios::binary);
    file.write(program_bytes.c_str(), program_bytes.size());
    file.close();
    LOG(INFO) << "serialize program to " << program_path;
  }
  ...

  {
    ProgramDesc program_desc;
    GraphToProgram(*graph, &program_desc);
    std::string program_bytes = program_desc.Proto()->SerializeAsString();
    std::string program_path = "_after_QuantDequantMkldnnPass.pdmodel";
    std::ofstream file(program_path.c_str(), std::ios::binary);
    file.write(program_bytes.c_str(), program_bytes.size());
    file.close();
    LOG(INFO) << "serialize program to " << program_path;
  }
}
```

## 2.1 quant_dequant_mkldnn_pass
这个 pass 主要会删除量化op；
对于新的量化格式 (dequantize_linear 和 quantize_linear op)， 模型的 topo 结构有些不同，quantize_linear 和 dequantize_linear 做模拟量化，收集 scale 信息（bias 信息一般都是 0， 对称量化）。 此外保存模型的权重数据已经是 int8 数据了。

对于中间激活数据（输入输出），只需要得到一个 scale 就行。 而权重数据是按照 channel 量化的，所有 scale 个数等于 channel 数。