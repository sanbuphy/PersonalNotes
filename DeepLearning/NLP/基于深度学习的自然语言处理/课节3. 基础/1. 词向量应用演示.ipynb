{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaddleNLP词向量应用展示\n",
    "\n",
    "6.7日NLP直播打卡课开始啦\n",
    "\n",
    "**[直播链接请戳这里，每晚20:00-21:30👈](http://live.bilibili.com/21689802)**\n",
    "\n",
    "**[课程地址请戳这里👈](https://aistudio.baidu.com/aistudio/course/introduce/24177)**\n",
    "\n",
    "欢迎来课程**QQ群**（群号:618354318）交流吧~~\n",
    "\n",
    "\n",
    "词向量（Word embedding），即把词语表示成实数向量。“好”的词向量能体现词语直接的相近关系。词向量已经被证明可以提高NLP任务的性能，例如语法分析和情感分析。\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/54878855b1df42f9ab50b280d76906b1e0175f280b0f4a2193a542c72634a9bf\" width=\"60%\" height=\"50%\"> <br />\n",
    "</p>\n",
    "<br><center>图1：词向量示意图</center></br>\n",
    "\n",
    "PaddleNLP已预置多个公开的预训练Embedding，您可以通过使用`paddlenlp.embeddings.TokenEmbedding`接口加载各种预训练Embedding。本篇教程将介绍`paddlenlp.embeddings.TokenEmbedding`的使用方法，计算词与词之间的语义距离，并结合词袋模型获取句子的语义表示。\n",
    "\n",
    "token：模型输入基本单元。比如中文BERT中，token可以是一个字，也可以是<CLS>等标识符，一般情况下 token 是 word\n",
    "\n",
    "embedding：一个用来表示token的稠密的向量。token本身不可计算，需要将其映射到一个连续向量空间，才可以进行后续运算，这个映射的结果就是该token对应的embedding。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载TokenEmbedding\n",
    "\n",
    "`TokenEmbedding()`参数\n",
    "- `embedding_name`\n",
    "将模型名称以参数形式传入TokenEmbedding，加载对应的模型。默认为`w2v.baidu_encyclopedia.target.word-word.dim300`的词向量。\n",
    "- `unknown_token`\n",
    "未知token的表示，默认为[UNK]。\n",
    "- `unknown_token_vector`\n",
    "未知token的向量表示，默认生成和embedding维数一致，数值均值为0的正态分布向量。\n",
    "- `extended_vocab_path`\n",
    "扩展词汇列表文件路径，词表格式为一行一个词。如引入扩展词汇列表，trainable=True。\n",
    "- `trainable`\n",
    "Embedding层是否可被训练。True表示Embedding可以更新参数，False为不可更新。默认为True。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paddlenlp.embeddings import TokenEmbedding\n",
    "\n",
    "# 初始化TokenEmbedding， 预训练embedding未下载时会自动下载并加载数据\n",
    "token_embedding = TokenEmbedding(embedding_name=\"w2v.baidu_encyclopedia.target.word-word.dim300\")\n",
    "\n",
    "# 查看token_embedding详情\n",
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 认识一下Embedding\n",
    "**`TokenEmbedding.search()`**\n",
    "获得指定词汇的词向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token_embedding = token_embedding.search(\"中国\")\n",
    "print(type(test_token_embedding))\n",
    "import paddle\n",
    "test_token_embedding_tensor = paddle.to_tensor(test_token_embedding)\n",
    "print(test_token_embedding_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`TokenEmbedding.cosine_sim()`**\n",
    "计算词向量间余弦相似度，语义相近的词语余弦相似度更高，说明预训练好的词向量空间有很好的语义表示能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score1 = token_embedding.cosine_sim(\"女孩\", \"女人\")\n",
    "score2 = token_embedding.cosine_sim(\"女孩\", \"书籍\")\n",
    "print('score1:', score1)\n",
    "print('score2:', score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于TokenEmbedding衡量句子语义相似度\n",
    "\n",
    "在许多实际应用场景（如文档检索系统）中， 需要衡量两个句子的语义相似程度。此时我们可以使用词袋模型（Bag of Words，简称BoW）计算句子的语义向量。\n",
    "\n",
    "**首先**，将两个句子分别进行切词，并在TokenEmbedding中查找相应的单词词向量（word embdding）。\n",
    "\n",
    "**然后**，根据词袋模型，将句子的word embedding叠加作为句子向量（sentence embedding）。\n",
    "\n",
    "**最后**，计算两个句子向量的余弦相似度。\n",
    "\n",
    "### 基于TokenEmbedding的词袋模型\n",
    "\n",
    "\n",
    "使用`BoWEncoder`搭建一个BoW模型用于计算句子语义。\n",
    "\n",
    "* `paddlenlp.TokenEmbedding`组建word-embedding层\n",
    "* `paddlenlp.seq2vec.BoWEncoder`组建句子建模层\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddlenlp\n",
    "\n",
    "\n",
    "class BoWModel(nn.Layer):\n",
    "    def __init__(self, embedder):\n",
    "        super().__init__()\n",
    "        self.embedder = embedder\n",
    "        emb_dim = self.embedder.embedding_dim\n",
    "        self.encoder = paddlenlp.seq2vec.BoWEncoder(emb_dim)\n",
    "        self.cos_sim_func = nn.CosineSimilarity(axis=-1)\n",
    "\n",
    "    def get_cos_sim(self, text_a, text_b):\n",
    "        text_a_embedding = self.forward(text_a)\n",
    "        text_b_embedding = self.forward(text_b)\n",
    "        cos_sim = self.cos_sim_func(text_a_embedding, text_b_embedding)\n",
    "        return cos_sim\n",
    "\n",
    "    def forward(self, text):\n",
    "        # Shape: (batch_size, num_tokens, embedding_dim)\n",
    "        embedded_text = self.embedder(text)\n",
    "        # Shape: (batch_size, embedding_dim)\n",
    "        summed = self.encoder(embedded_text)\n",
    "        return summed\n",
    "\n",
    "model = BoWModel(embedder=token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造Tokenizer\n",
    "使用TokenEmbedding词表构造Tokenizer。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "print(token_embedding)\n",
    "\n",
    "tokenizer.set_vocab(vocab=token_embedding.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相似句对数据读取\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pairs = {}\n",
    "with open(\"text_pair.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        text_a, text_b = line.strip().split(\"\\t\")\n",
    "        if text_a not in text_pairs:\n",
    "            text_pairs[text_a] = []\n",
    "        text_pairs[text_a].append(text_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看相似语句相关度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text_a, text_b_list in text_pairs.items():\n",
    "    text_a_ids = paddle.to_tensor([tokenizer.text_to_ids(text_a)])\n",
    "    embedded_text = token_embedding(text_a_ids)\n",
    "\n",
    "    for text_b in text_b_list:\n",
    "        text_b_ids = paddle.to_tensor([tokenizer.text_to_ids(text_b)])\n",
    "        print(\"text_a: {}\".format(text_a))\n",
    "        print(\"text_b: {}\".format(text_b))\n",
    "        print(\"cosine_sim: {}\".format(model.get_cos_sim(text_a_ids, text_b_ids).numpy()[0]))\n",
    "        print()\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
