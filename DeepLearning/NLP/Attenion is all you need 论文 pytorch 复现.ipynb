{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[参考：The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model  #表示embedding的维度\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/f0f23768fde44668862580e5121e4a418a456ef7082c47e0b7f57864c17e3898)\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/8ae4070f176e44fc9617562b8aef4ed9c41026cd5f8e4e63ab274b392af54cef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaled Dot-Product Attention\n",
    "'''\n",
    "注意力机制；可以参考网上的一些讲解： 如这里提供的：经典论文复现Transformer(理论篇)\n",
    "\n",
    "对于单头注意力机制，简单理解过程如下：下面论述中，小写的qkv 表示某一个词向量对应的qkv， 大写的QKV表示整个输入句子对应的QKV\n",
    "\n",
    "1. 输入是一句话，一句话中有很多单词，句子的长度为 length\n",
    "2. embedding：句子是文本，计算机无法直接处理，需要把每个字转换为词向量，这就是 word_embedding， 每个词向量的维度是 d_model\n",
    "3. 这样输入 input 的维度是： [batch, length, d_model]\n",
    "4. 每次词都会生成 Q,K,V; 其中 Wq,Wk,Wv 的 shape 为 [d_model, d_k], [d_model, d_k], [d_model, d_v]\n",
    "   Q = input * Wq\n",
    "   K = input * Wk\n",
    "   V = input * Wv\n",
    "  Q:[batch, length, d_k]; K:[batch, length, d_k]; V:[batch, length, d_v]\n",
    "5. 得到的 Q K V 就是上面第一个图的输入\n",
    "   Q 乘以 K 的transpose，然后除以 d_k 的算术平方根， 结果的 shape 为 [batch, length, lenght]\n",
    "   然后再乘以 V， 最后的shape 为 [batch, length, d_v]\n",
    "'''\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"计算Attention即点乘V\"\n",
    "    d_k = query.size(-1)\n",
    "    # 单头：输入：query: [B, L, d_k]; key: [B, L, d_k]； 输出：scores：[B, L, L]\n",
    "    # 多头：输入：query: [B, h, L, d_k]; key: [B, h, L, d_k]； 输出：scores：[B, h, L, L]\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "             / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    # 单头: scores: [B, L, L]; p_attn:[B, L, L]\n",
    "    # 多头: scores: [B, h, L, L]; p_attn:[B, h, L, L]\n",
    "    p_attn = F.softmax(scores, dim = -1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    # 单头: 输出1：[B, L, d_v]; 输出2 p_attn:[B, L, L]\n",
    "    # 多头: 输出1：[B, h, L, d_v]; 输出2 p_attn:[B, h, L, L]\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "'''\n",
    "但是论文里面的是多头注意力， 如第二幅图.\n",
    "如果是多头注意力:\n",
    "沿着上述单头注意力机制的理解：\n",
    "1. 输入 input 依然是 [batch, length, d_model]\n",
    "2. 每个词也都会生成 Q K V, 但由于是多头，所以 Q K V 的 shape 为 \n",
    "    Q:[batch, head, length, d_k], K:[batch, head, length, d_k], V:[batch, head, length, d_v]\n",
    "    d_model = length * d_k\n",
    "3. 然后Q 乘以 K 的transpose，然后除以 d_k 的算术平方根， 结果的 shape 为 [batch, head, length, lenght]\n",
    "   然后再乘以 V， 最后的shape 为 [batch, head, length, d_v]\n",
    "4. 最后还原成输入的 shape ： [batch, head, length, d_v] -> [batch, length, d_model]\n",
    "\n",
    "实际实现中，上述第二幅图中， Q,K,V对应的就是 input，和第一幅图中的 Q, K, V不一样，是为了方便计算，第一幅中的Q, K, V是为了方便大家理解。\n",
    "Q, K, V都是输入，然后经过 linear 层，才得到上面所说的shape 为 [batch, head, length, d_k]的 Q,K,V\n",
    "\n",
    "所以论文中第二幅图中的Q,K,V 都是输入， 对应上述中的第一步；\n",
    "经过linear层后，得到上述第二步中的shape为 [batch, head, length, d_k] 的Q,K,V\n",
    "这样做的目的是封装性更好。\n",
    "\n",
    "'''\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        实现MultiHeadedAttention。\n",
    "           输入的Q,K,V是形状 [batch, L, d_model],本质上就是输入句子的词向量\n",
    "           输出的形状同上。\n",
    "    \"\"\"\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads.\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"Implements Figure 2\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h * d_k\n",
    "        # 这一步QKV变化:[batch, L, d_model] -> [batch, h, L, d_k]\n",
    "        # 本质上就是计算输入每一个单词的q,k,v, d_model = h * d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        # QKV :[batch, h, L, d_model/h] -->x:[b, h, L, d_v], attn[b, h, L, L]\n",
    "        x, self.attn = attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        # 上一步的结果合并在一起还原成原始输入序列的形状\n",
    "        x = ( # [b, h, L, d_v]\n",
    "            x.transpose(1, 2) # [b, L, h, d_v]\n",
    "            .contiguous()\n",
    "            .view(nbatches, -1, self.h * self.d_k) # [b, L, d_model]\n",
    "        )\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        # 最后再过一个线性层\n",
    "        return self.linears[-1](x) # [b, L, d_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (be equal to nn.LayerNorm).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
